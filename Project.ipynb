{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the input Excel file\n",
    "input_file = 'input.xlsx'\n",
    "workbook = openpyxl.load_workbook(input_file)\n",
    "sheet = workbook.active\n",
    "\n",
    "# Create a list to store the extracted articles\n",
    "articles = []\n",
    "\n",
    "# Iterate over the rows in the Excel file\n",
    "for row in sheet.iter_rows(values_only=True):\n",
    "    url_id = row[0]\n",
    "    html_content = row[1]\n",
    "    \n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Extract the article text (assuming the article is contained in a <div> with class \"article\")\n",
    "    article_div = soup.find('div', class_='article')\n",
    "    article_text = article_div.get_text(strip=True) if article_div else ''\n",
    "    \n",
    "    # Append the URL_ID and article text to the list\n",
    "    articles.append((url_id, article_text))\n",
    "\n",
    "# Close the input Excel file\n",
    "workbook.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94de8861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import re\n",
    "import math\n",
    "\n",
    "# Function to calculate the FOG index\n",
    "def calculate_fog_index(avg_sentence_length, percentage_complex_words):\n",
    "    return 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "# Function to count the number of syllables in a word\n",
    "def count_syllables(word):\n",
    "    # Source: https://eayd.in/?p=232\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower().strip(\".:;?!\")\n",
    "    if word[0] in vowels:\n",
    "        word = f\" {word}\"\n",
    "    return len(re.findall('[^aeiouy][aeiouy]', word))\n",
    "\n",
    "# Create a list to store the computed variables for each article\n",
    "computed_variables = []\n",
    "\n",
    "# Iterate over the extracted articles\n",
    "for url_id, article_text in articles:\n",
    "    # Create a TextBlob object for the article text\n",
    "    blob = TextBlob(article_text)\n",
    "    \n",
    "    # Compute the required variables\n",
    "    positive_score = blob.sentiment.polarity\n",
    "    negative_score = -positive_score\n",
    "    polarity_score = blob.sentiment.polarity\n",
    "    subjectivity_score = blob.sentiment.subjectivity\n",
    "    \n",
    "    sentences = blob.sentences\n",
    "    num_sentences = len(sentences)\n",
    "    num_words = len(blob.words)\n",
    "    num_syllables = sum(count_syllables(word) for word in blob.words)\n",
    "    num_complex_words = sum(1 for word in blob.words if count_syllables(word) > 2)\n",
    "    \n",
    "    avg_sentence_length = num_words / num_sentences\n",
    "    percentage_complex_words = (num_complex_words / num_words) * 100\n",
    "    fog_index = calculate_fog_index(avg_sentence_length, percentage_complex_words)\n",
    "    avg_words_per_sentence = num_words / num_sentences\n",
    "    avg_word_length = sum(len(word) for word in blob.words) / num_words\n",
    "    \n",
    "    personal_pronouns = len([word for word, pos in blob.tags if pos == 'PRP' or pos == 'PRP$'])\n",
    "    \n",
    "    # Append the computed variables to the list\n",
    "    computed_variables.append((\n",
    "        url_id, positive_score, negative_score, polarity_score, subjectivity_score,\n",
    "        avg_sentence_length, percentage_complex_words, fog_index,\n",
    "        avg_words_per_sentence, num_complex_words, num_words, num_syllables,\n",
    "        personal_pronouns, avg_word_length\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734274b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the output structure Excel file\n",
    "output_file = 'Output Data Structure.xlsx'\n",
    "output_workbook = openpyxl.load_workbook(output_file)\n",
    "output_sheet = output_workbook.active\n",
    "\n",
    "# Write the computed variables to the output Excel file\n",
    "for i, data in enumerate(computed_variables, start=2):\n",
    "    output_sheet.cell(row=i, column=1, value=data[0])  # URL_ID\n",
    "    \n",
    "    # Write the computed variables in the specified order\n",
    "    for j, value in enumerate(data[1:], start=2):\n",
    "        output_sheet.cell(row=i, column=j, value=value)\n",
    "\n",
    "# Save and close the output Excel file\n",
    "output_workbook.save('output.xlsx')\n",
    "output_workbook.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67515d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "# Load the CMU pronunciation dictionary for syllable count\n",
    "nltk.download('cmudict')\n",
    "pronunciation_dict = cmudict.dict()\n",
    "\n",
    "# Load the input Excel file\n",
    "input_file = 'input.xlsx'\n",
    "workbook = openpyxl.load_workbook(input_file)\n",
    "sheet = workbook.active\n",
    "\n",
    "# Create a list to store the extracted articles\n",
    "articles = []\n",
    "\n",
    "# Iterate over the rows in the Excel file\n",
    "for row in sheet.iter_rows(values_only=True):\n",
    "    url_id = row[0]\n",
    "    html_content = row[1]\n",
    "    \n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Extract the article text (assuming the article is contained in a <div> with class \"article\")\n",
    "    article_div = soup.find('div', class_='article')\n",
    "    article_text = article_div.get_text(strip=True) if article_div else ''\n",
    "    \n",
    "    # Append the URL_ID and article text to the list\n",
    "    articles.append((url_id, article_text))\n",
    "\n",
    "# Close the input Excel file\n",
    "workbook.close()\n",
    "\n",
    "# Create a SentimentIntensityAnalyzer object\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Create a list to store the computed variables for each article\n",
    "computed_variables = []\n",
    "\n",
    "# Iterate over the extracted articles\n",
    "for url_id, article_text in articles:\n",
    "    # Compute the sentiment scores\n",
    "    sentiment_scores = sia.polarity_scores(article_text)\n",
    "    \n",
    "    # Tokenize the article into sentences and words\n",
    "    sentences = sent_tokenize(article_text)\n",
    "    words = word_tokenize(article_text)\n",
    "    \n",
    "    num_sentences = len(sentences)\n",
    "    num_words = len(words)\n",
    "    \n",
    "    # Compute the number of syllables for each word\n",
    "    num_syllables = sum([len(pronunciation_dict.get(word.lower(), [[]]))[0] for word in words])\n",
    "    \n",
    "    # Compute the number of complex words (words with more than two syllables)\n",
    "    num_complex_words = sum([1 for word in words if len(pronunciation_dict.get(word.lower(), [[]]))[0] > 2])\n",
    "    \n",
    "    # Compute the average sentence length (number of words per sentence)\n",
    "    avg_sentence_length = num_words / num_sentences\n",
    "    \n",
    "    # Compute the percentage of complex words\n",
    "    percentage_complex_words = (num_complex_words / num_words) * 100\n",
    "    \n",
    "    # Compute the FOG index\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    \n",
    "    # Compute the average number of words per sentence\n",
    "    avg_words_per_sentence = num_words / num_sentences\n",
    "    \n",
    "    # Compute the average word length\n",
    "    avg_word_length = sum(len(word) for word in words) / num_words\n",
    "    \n",
    "    # Compute the number of personal pronouns (using pattern matching)\n",
    "    personal_pronouns = len(re.findall(r'\\b(I|me|my|mine|we|us|our|ours|you|your|yours)\\b', article_text, re.IGNORECASE))\n",
    "    \n",
    "    # Append the computed variables to the list\n",
    "    computed_variables.append((\n",
    "        url_id, sentiment_scores['pos'], sentiment_scores['neg'], sentiment_scores['compound'],\n",
    "        sentiment_scores['compound'], avg_sentence_length, percentage_complex_words,\n",
    "        fog_index, avg_words_per_sentence, num_complex_words, num_words, num_syllables,\n",
    "        personal_pronouns, avg_word_length\n",
    "    ))\n",
    "\n",
    "# Load the output structure Excel file\n",
    "output_file = 'Output Data Structure.xlsx'\n",
    "output_workbook = openpyxl.load_workbook(output_file)\n",
    "output_sheet = output_workbook.active\n",
    "\n",
    "# Write the computed variables to the output Excel file\n",
    "for i, data in enumerate(computed_variables, start=2):\n",
    "    output_sheet.cell(row=i, column=1, value=data[0])  # URL_ID\n",
    "    \n",
    "    # Write the computed variables in the specified order\n",
    "    for j, value in enumerate(data[1:], start=2):\n",
    "        output_sheet.cell(row=i, column=j, value=value)\n",
    "\n",
    "# Save and close the output Excel file\n",
    "output_workbook.save('output.xlsx')\n",
    "output_workbook.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0653f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43386b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb3eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Step 1: Read the input Excel file using pandas\n",
    "input_file = \"input.xlsx\"\n",
    "output_file = \"output.xlsx\"\n",
    "input_data = pd.read_excel(input_file)\n",
    "\n",
    "# Create an empty DataFrame to store the computed variables\n",
    "output_data = pd.DataFrame(columns=[\n",
    "    \"URL_ID\", \"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\", \"SUBJECTIVITY SCORE\",\n",
    "    \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\",\n",
    "    \"AVG NUMBER OF WORDS PER SENTENCE\", \"COMPLEX WORD COUNT\", \"WORD COUNT\",\n",
    "    \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\", \"AVG WORD LENGTH\"\n",
    "])\n",
    "\n",
    "# Loop through each row of the input file\n",
    "for index, row in input_data.iterrows():\n",
    "    url_id = row[\"URL_ID\"]\n",
    "    article_url = row[\"Article_URL\"]\n",
    "\n",
    "    # Step 2: Extract the article text using requests and BeautifulSoup libraries\n",
    "    response = requests.get(article_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Extract the article title and text\n",
    "    article_title = soup.find(\"title\").text\n",
    "    article_text = soup.find(\"body\").text\n",
    "\n",
    "    # Step 3: Clean the extracted text by removing unwanted characters, headers, footers, etc.\n",
    "    # Add your cleaning logic here\n",
    "\n",
    "    # Step 4: Perform textual analysis on the cleaned text\n",
    "    blob = TextBlob(article_text)\n",
    "    positive_score = blob.sentiment.polarity\n",
    "    negative_score = -positive_score\n",
    "    polarity_score = blob.sentiment.polarity\n",
    "    subjectivity_score = blob.sentiment.subjectivity\n",
    "    avg_sentence_length = blob.sentences.mean(lambda sentence: len(sentence.split()))\n",
    "    words = blob.words\n",
    "    total_words = len(words)\n",
    "    complex_words = [word for word in words if len(word) > 2 and len(word.syllables) > 2]\n",
    "    complex_word_count = len(complex_words)\n",
    "    percentage_complex_words = (complex_word_count / total_words) * 100\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    avg_words_per_sentence = total_words / len(blob.sentences)\n",
    "    syllable_count = sum(len(word.syllables) for word in words) / total_words\n",
    "    personal_pronouns = sum(1 for word in words if word.lower() in [\"i\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\"])\n",
    "    avg_word_length = sum(len(word) for word in words) / total_words\n",
    "\n",
    "    # Step 5: Store the computed variables in the output DataFrame\n",
    "    output_data = output_data.append({\n",
    "        \"URL_ID\": url_id,\n",
    "        \"POSITIVE SCORE\": positive_score,\n",
    "        \"NEGATIVE SCORE\": negative_score,\n",
    "        \"POLARITY SCORE\": polarity_score,\n",
    "        \"SUBJECTIVITY SCORE\": subjectivity_score,\n",
    "        \"AVG SENTENCE LENGTH\": avg_sentence_length,\n",
    "        \"PERCENTAGE OF COMPLEX WORDS\": percentage_complex_words,\n",
    "        \"FOG INDEX\": fog_index,\n",
    "        \"AVG NUMBER OF WORDS PER SENTENCE\": avg_words_per_sentence,\n",
    "        \"COMPLEX WORD COUNT\": complex_word_count,\n",
    "        \"WORD COUNT\": total_words,\n",
    "        \"SYLLABLE PER WORD\": syllable_count,\n",
    "        \"PERSONAL PRONOUNS\": personal_pronouns,\n",
    "        \"AVG WORD LENGTH\": avg_word_length\n",
    "    }, ignore_index=True)\n",
    "\n",
    "# Step 6: Save the DataFrame to an output Excel file\n",
    "output_data.to_excel(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe60812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "# Step 1: Data Extraction\n",
    "input_file = \"input.xlsx\"\n",
    "output_file = \"Output Data Structure.xlsx\"\n",
    "\n",
    "# Read input file\n",
    "input_data = pd.read_excel(input_file)\n",
    "\n",
    "# Iterate over URLs and extract article text\n",
    "for index, row in input_data.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract article title and text\n",
    "    article_title = soup.find('h1').text\n",
    "    article_text = soup.find('article').text\n",
    "\n",
    "    # Save the extracted article in a text file\n",
    "    with open(f\"{url_id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(article_title + \"\\n\" + article_text)\n",
    "\n",
    "# Step 2: Data Analysis\n",
    "# Load CMU Pronouncing Dictionary for syllable count\n",
    "cmud = cmudict.dict()\n",
    "\n",
    "# Read the extracted articles and perform analysis\n",
    "output_data = []\n",
    "for index, row in input_data.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    file_name = f\"{url_id}.txt\"\n",
    "\n",
    "    # Read the article text from the text file\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "        article_text = file.read()\n",
    "\n",
    "    # Perform text analysis using TextBlob\n",
    "    blob = TextBlob(article_text)\n",
    "\n",
    "    # Calculate variables\n",
    "    positive_score = blob.sentiment.polarity\n",
    "    negative_score = -blob.sentiment.polarity\n",
    "    polarity_score = blob.sentiment.polarity\n",
    "    subjectivity_score = blob.sentiment.subjectivity\n",
    "\n",
    "    sentences = blob.sentences\n",
    "    word_count = len(blob.words)\n",
    "    sentence_count = len(sentences)\n",
    "    syllable_count = sum([len(cmud[word.lower()][0]) for word in blob.words if word.lower() in cmud])\n",
    "\n",
    "    avg_sentence_length = word_count / sentence_count\n",
    "    complex_words_count = len([word for word in blob.words if len(cmud[word.lower()]) > 1])\n",
    "    percentage_complex_words = (complex_words_count / word_count) * 100\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "    avg_words_per_sentence = word_count / sentence_count\n",
    "    syllables_per_word = syllable_count / word_count\n",
    "    personal_pronouns = blob.word_counts['i'] + blob.word_counts['me'] + blob.word_counts['my'] + blob.word_counts['mine']\n",
    "\n",
    "    avg_word_length = sum(len(word) for word in blob.words) / word_count\n",
    "\n",
    "    # Add the variables to the output data list\n",
    "    output_data.append([url_id, positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length,\n",
    "                        percentage_complex_words, fog_index, avg_words_per_sentence, complex_words_count, word_count,\n",
    "                        syllables_per_word, personal_pronouns, avg_word_length])\n",
    "\n",
    "# Step 3: Save the output data to an Excel file\n",
    "output_df = pd.DataFrame(output_data, columns=['URL_ID', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
    "                                               'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS',\n",
    "                                               'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT',\n",
    "                                               'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'])\n",
    "output_df.to_excel(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b3abf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not extract content from URL: https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/what-if-the-creation-is-taking-over-the-creator/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/what-jobs-will-robots-take-from-humans-in-the-future/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/will-machine-replace-the-human-in-the-future-of-work/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/will-ai-replace-us-or-work-with-us/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/man-and-machines-together-machines-are-more-diligent-than-humans-blackcoffe/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/in-future-or-in-upcoming-years-humans-and-machines-are-going-to-work-together-in-every-field-of-work/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-machine-learning-will-affect-your-business/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/deep-learning-impact-on-areas-of-e-learning/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-to-protect-future-data-and-its-privacy-blackcoffer/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-machines-ai-automations-and-robo-human-are-effective-in-finance-and-banking/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/ai-human-robotics-machine-future-planet-blackcoffer-thinking-jobs-workplace/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-ai-will-change-the-world-blackcoffer/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/future-of-work-how-ai-has-entered-the-workplace/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/ai-tool-alexa-google-assistant-finance-banking-tool-future/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/ai-healthcare-revolution-ml-technology-algorithm-google-analytics-industrialrevolution/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/all-you-need-to-know-about-online-marketing/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/evolution-of-advertising-industry/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-data-analytics-can-help-your-business-respond-to-the-impact-of-covid-19/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/environmental-impact-of-the-covid-19-pandemic-lesson-for-the-future/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-data-analytics-and-ai-are-used-to-halt-the-covid-19-pandemic/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/difference-between-artificial-intelligence-machine-learning-statistics-and-data-mining/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-python-became-the-first-choice-for-data-science/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-google-fit-measure-heart-and-respiratory-rates-using-a-phone/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/what-is-the-future-of-mobile-apps/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/impact-of-ai-in-health-and-medicine/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/telemedicine-what-patients-like-and-dislike-about-it/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-we-forecast-future-technologies/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/can-robots-tackle-late-life-loneliness/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/embedding-care-robots-into-society-socio-technical-considerations/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/management-challenges-for-future-digitalization-of-healthcare-services/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/are-we-any-closer-to-preventing-a-nuclear-holocaust/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/will-technology-eliminate-the-need-for-animal-testing-in-drug-development/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/will-we-ever-understand-the-nature-of-consciousness/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/will-we-ever-colonize-outer-space/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/what-is-the-chance-homo-sapiens-will-survive-for-the-next-500-years/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/why-does-your-business-need-a-chatbot/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-you-lead-a-project-or-a-team-without-any-technical-expertise/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/can-you-be-great-leader-without-technical-expertise/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-does-artificial-intelligence-affect-the-environment/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-to-overcome-your-fear-of-making-mistakes-2/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/is-perfection-the-greatest-enemy-of-productivity/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/global-financial-crisis-2008-causes-effects-and-its-solution/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/gender-diversity-and-equality-in-the-tech-industry/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-to-overcome-your-fear-of-making-mistakes/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-small-business-can-survive-the-coronavirus-crisis/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/impacts-of-covid-19-on-vegetable-vendors-and-food-stalls/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/impacts-of-covid-19-on-vegetable-vendors/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/impact-of-covid-19-pandemic-on-tourism-aviation-industries/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/impact-of-covid-19-pandemic-on-sports-events-around-the-world/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/changing-landscape-and-emerging-trends-in-the-indian-it-ites-industry/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/online-gaming-adolescent-online-gaming-effects-demotivated-depression-musculoskeletal-and-psychosomatic-symptoms/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/human-rights-outlook/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-voice-search-makes-your-business-a-successful-business/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-the-covid-19-crisis-is-redefining-jobs-and-services/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-to-increase-social-media-engagement-for-marketers/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/impacts-of-covid-19-on-streets-sides-food-stalls/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/coronavirus-impact-on-energy-markets-2/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry-5/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis-4/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/estimating-the-impact-of-covid-19-on-the-world-of-work-2/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/estimating-the-impact-of-covid-19-on-the-world-of-work-3/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/travel-and-tourism-outlook/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/gaming-disorder-and-effects-of-gaming-on-health/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not extract content from URL: https://insights.blackcoffer.com/what-is-the-repercussion-of-the-environment-due-to-the-covid-19-pandemic-situation/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/what-is-the-repercussion-of-the-environment-due-to-the-covid-19-pandemic-situation-2/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/impact-of-covid-19-pandemic-on-office-space-and-co-working-industries/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/contribution-of-handicrafts-visual-arts-literature-in-the-indian-economy/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-covid-19-is-impacting-payment-preferences/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-2/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/covid-19-how-have-countries-been-responding/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry-2/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-3/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry-3/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/estimating-the-impact-of-covid-19-on-the-world-of-work/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/covid-19-how-have-countries-been-responding-2/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-4/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis-2/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/lessons-from-the-past-some-key-learnings-relevant-to-the-coronavirus-crisis-3/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry-4/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/why-scams-like-nirav-modi-happen-with-indian-banks/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/impact-of-covid-19-on-the-global-economy/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/impact-of-covid-19coronavirus-on-the-indian-economy-2/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/impact-of-covid-19-on-the-global-economy-2/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/impact-of-covid-19-coronavirus-on-the-indian-economy-3/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/should-celebrities-be-allowed-to-join-politics/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-prepared-is-india-to-tackle-a-possible-covid-19-outbreak/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/controversy-as-a-marketing-strategy/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/coronavirus-impact-on-the-hospitality-industry/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/coronavirus-impact-on-energy-markets/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/what-are-the-key-policies-that-will-mitigate-the-impacts-of-covid-19-on-the-world-of-work/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/marketing-drives-results-with-a-focus-on-problems/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/continued-demand-for-sustainability/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/coronavirus-disease-covid-19-effect-the-impact-and-role-of-mass-media-during-the-pandemic/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/should-people-wear-fabric-gloves-seeking-evidence-regarding-the-differential-transfer-of-covid-19-or-coronaviruses-generally-between-surfaces/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/why-is-there-a-severe-immunological-and-inflammatory-explosion-in-those-affected-by-sarms-covid-19/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/what-do-you-think-is-the-lesson-or-lessons-to-be-learned-with-covid-19/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/coronavirus-the-unexpected-challenge-for-the-european-union/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/industrial-revolution-4-0-pros-and-cons/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/impact-of-covid-19-coronavirus-on-the-indian-economy/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/impact-of-covid-19-coronavirus-on-the-indian-economy-2/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/impact-of-covid-19coronavirus-on-the-indian-economy/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/impact-of-covid-19-coronavirus-on-the-global-economy/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/blockchain-in-fintech/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/blockchain-for-payments/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/the-future-of-investing/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/big-data-analytics-in-healthcare/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/business-analytics-in-the-healthcare-industry/\n",
      "Could not extract content from URL: https://insights.blackcoffer.com/challenges-and-opportunities-of-big-data-in-healthcare/\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'syllable_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m sentences \u001b[38;5;241m=\u001b[39m blob\u001b[38;5;241m.\u001b[39msentences\n\u001b[0;32m     64\u001b[0m word_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(blob\u001b[38;5;241m.\u001b[39mwords)\n\u001b[1;32m---> 65\u001b[0m cmud[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msyllable_count\u001b[49m\n\u001b[0;32m     66\u001b[0m sentence_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sentences)\n\u001b[0;32m     67\u001b[0m syllable_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mlen\u001b[39m(cmud[word\u001b[38;5;241m.\u001b[39mlower()][\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m blob\u001b[38;5;241m.\u001b[39mwords \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m cmud])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'syllable_count' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "# Step 1: Data Extraction\n",
    "input_file = \"input.xlsx\"\n",
    "output_file = \"Output Data Structure.xlsx\"\n",
    "\n",
    "# Read input file\n",
    "input_data = pd.read_excel(input_file)\n",
    "\n",
    "# Iterate over URLs and extract article text\n",
    "for index, row in input_data.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the article content based on specific selectors or techniques\n",
    "    # Modify the selectors or techniques based on the HTML structure of the articles\n",
    "    article_content = soup.find('div', class_='article-content')  # Example selector\n",
    "\n",
    "    # Extract article title and text\n",
    "    if article_content:\n",
    "        article_title = article_content.find('h1').text.strip()\n",
    "        article_text = article_content.find('div', class_='article-text').text.strip()\n",
    "\n",
    "        # Save the extracted article in a text file\n",
    "        with open(f\"{url_id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(article_title + \"\\n\" + article_text)\n",
    "    else:\n",
    "        print(f\"Could not extract content from URL: {url}\")\n",
    "\n",
    "# Step 2: Data Analysis\n",
    "# Load CMU Pronouncing Dictionary for syllable count\n",
    "cmud = cmudict.dict()\n",
    "\n",
    "# Read the extracted articles and perform analysis\n",
    "output_data = []\n",
    "for index, row in input_data.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    file_name = f\"{url_id}.txt\"\n",
    "\n",
    "    # Read the article text from the text file\n",
    "    with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "        article_text = file.read()\n",
    "\n",
    "    # Perform text analysis using TextBlob\n",
    "    blob = TextBlob(article_text)\n",
    "\n",
    "    # Calculate variables\n",
    "    positive_score = blob.sentiment.polarity\n",
    "    negative_score = -blob.sentiment.polarity\n",
    "    polarity_score = blob.sentiment.polarity\n",
    "    subjectivity_score = blob.sentiment.subjectivity\n",
    "    sentences = blob.sentences\n",
    "    word_count = len(blob.words)\n",
    "    \n",
    "    sentence_count = len(sentences)\n",
    "    syllable_count = sum([len(cmud[word.lower()][0]) for word in blob.words if word.lower() in cmud])\n",
    "    cmud['word'] = syllable_count\n",
    "    \n",
    "    avg_sentence_length = word_count / sentence_count\n",
    "    complex_words_count = len([word for word in blob.words if len(cmud[word.lower()]) > 1])\n",
    "    percentage_complex_words = (complex_words_count / word_count) * 100\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "    avg_words_per_sentence = word_count / sentence_count\n",
    "    syllables_per_word = syllable_count / word_count\n",
    "    personal_pronouns = blob.word_counts['i'] + blob.word_counts['me'] + blob.word_counts['my'] + blob.word_counts['mine']\n",
    "\n",
    "    avg_word_length = sum(len(word) for word in blob.words) / word_count\n",
    "\n",
    "    # Add the variables to the output data list\n",
    "    output_data.append([url_id, positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length,\n",
    "                        percentage_complex_words, fog_index, avg_words_per_sentence, complex_words_count, word_count,\n",
    "                        syllables_per_word, personal_pronouns, avg_word_length])\n",
    "\n",
    "# Step 3: Save the output data to an Excel file\n",
    "output_df = pd.DataFrame(output_data, columns=['URL_ID', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
    "                                               'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS',\n",
    "                                               'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT',\n",
    "                                               'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'])\n",
    "output_df.to_excel(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e230d4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
